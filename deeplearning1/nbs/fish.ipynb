{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path = \"data/fish/sample/\"\n",
    "path = \"data/fish/\"\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3277 images belonging to 8 classes.\n",
      "Found 500 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(path+'train', batch_size=batch_size)\n",
    "val_batches = get_batches(path+'valid', batch_size=batch_size*2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Setup dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%cd data/fish\n",
    "\n",
    "%cd train\n",
    "\n",
    "%mkdir ../valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = glob('*')\n",
    "for d in g: os.mkdir('../valid/'+d)\n",
    "\n",
    "g = glob('*/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(500): os.rename(shuf[i], '../valid/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%mkdir ../sample\n",
    "%mkdir ../sample/train\n",
    "%mkdir ../sample/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "g = glob('*')\n",
    "for d in g: \n",
    "    os.mkdir('../sample/train/'+d)\n",
    "    os.mkdir('../sample/valid/'+d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = glob('*/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(400): copyfile(shuf[i], '../sample/train/' + shuf[i])\n",
    "\n",
    "%cd ../valid\n",
    "\n",
    "g = glob('*/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(200): copyfile(shuf[i], '../sample/valid/' + shuf[i])\n",
    "\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/jhoward/fast-image/nbs/data/fish\n"
     ]
    }
   ],
   "source": [
    "%mkdir results\n",
    "\n",
    "%mkdir sample/results\n",
    "\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vgg16bn import Vgg16BN\n",
    "model = vgg_ft_bn(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3277 images belonging to 8 classes.\n",
      "Found 500 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "trn = get_data(path+'train')\n",
    "val = get_data(path+'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test = get_data(path+'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/trn.dat', trn)\n",
    "save_array(path+'results/val.dat', val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/test.dat', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trn = load_array(path+'results/trn.dat')\n",
    "val = load_array(path+'results/val.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = load_array(path+'results/test.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3277 images belonging to 8 classes.\n",
      "Found 500 images belonging to 8 classes.\n",
      "Found 1000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen = image.ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(1e-3),\n",
    "       loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/3\n",
      "3277/3277 [==============================] - 87s - loss: 2.9412 - acc: 0.4760 - val_loss: 1.1016 - val_acc: 0.7060\n",
      "Epoch 2/3\n",
      "3277/3277 [==============================] - 87s - loss: 1.5476 - acc: 0.6460 - val_loss: 0.5617 - val_acc: 0.8380\n",
      "Epoch 3/3\n",
      "3277/3277 [==============================] - 87s - loss: 1.2630 - acc: 0.7009 - val_loss: 0.4466 - val_acc: 0.8700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa656f0e210>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, trn_labels, batch_size=batch_size, nb_epoch=3, validation_data=(val, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights(path+'results/ft1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(path+'results/ft1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_layers,fc_layers = split_at(model, Convolution2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_feat = conv_model.predict(trn)\n",
    "conv_val_feat = conv_model.predict(val)\n",
    "conv_test_feat = conv_model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/conv_val_feat.dat', conv_val_feat)\n",
    "save_array(path+'results/conv_test_feat.dat', conv_test_feat)\n",
    "save_array(path+'results/conv_feat.dat', conv_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_feat = load_array(path+'results/conv_feat.dat')\n",
    "conv_val_feat = load_array(path+'results/conv_val_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 512, 14, 14)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_val_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(8, activation='softmax')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/1\n",
      "3277/3277 [==============================] - 0s - loss: 2.5376 - acc: 0.3558 - val_loss: 1.1038 - val_acc: 0.7660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f751c31e250>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=1, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3277 samples, validate on 500 samples\n",
      "Epoch 1/15\n",
      "3277/3277 [==============================] - 0s - loss: 1.2942 - acc: 0.6378 - val_loss: 0.4884 - val_acc: 0.8660\n",
      "Epoch 2/15\n",
      "3277/3277 [==============================] - 0s - loss: 0.8186 - acc: 0.7608 - val_loss: 0.4009 - val_acc: 0.9080\n",
      "Epoch 3/15\n",
      "3277/3277 [==============================] - 0s - loss: 0.5820 - acc: 0.8267 - val_loss: 0.3173 - val_acc: 0.9240\n",
      "Epoch 4/15\n",
      "3277/3277 [==============================] - 0s - loss: 0.4586 - acc: 0.8648 - val_loss: 0.2578 - val_acc: 0.9420\n",
      "Epoch 5/15\n",
      "3277/3277 [==============================] - 0s - loss: 0.3995 - acc: 0.8785 - val_loss: 0.2237 - val_acc: 0.9460\n",
      "Epoch 6/15\n",
      "3277/3277 [==============================] - 0s - loss: 0.3008 - acc: 0.9078 - val_loss: 0.1888 - val_acc: 0.9480\n",
      "Epoch 7/15\n",
      "3277/3277 [==============================] - 0s - loss: 0.2706 - acc: 0.9182 - val_loss: 0.2009 - val_acc: 0.9560\n",
      "Epoch 8/15\n",
      "3277/3277 [==============================] - 0s - loss: 0.2350 - acc: 0.9307 - val_loss: 0.1837 - val_acc: 0.9540\n",
      "Epoch 9/15\n",
      "3277/3277 [==============================] - 0s - loss: 0.2294 - acc: 0.9374 - val_loss: 0.1792 - val_acc: 0.9560\n",
      "Epoch 10/15\n",
      "3277/3277 [==============================] - 0s - loss: 0.2151 - acc: 0.9390 - val_loss: 0.1555 - val_acc: 0.9580\n",
      "Epoch 11/15\n",
      "3277/3277 [==============================] - 0s - loss: 0.1721 - acc: 0.9500 - val_loss: 0.1725 - val_acc: 0.9560\n",
      "Epoch 12/15\n",
      "3277/3277 [==============================] - 0s - loss: 0.1490 - acc: 0.9530 - val_loss: 0.1737 - val_acc: 0.9580\n",
      "Epoch 13/15\n",
      "3277/3277 [==============================] - 0s - loss: 0.1455 - acc: 0.9524 - val_loss: 0.1504 - val_acc: 0.9660\n",
      "Epoch 14/15\n",
      "3277/3277 [==============================] - 0s - loss: 0.1139 - acc: 0.9640 - val_loss: 0.1432 - val_acc: 0.9680\n",
      "Epoch 15/15\n",
      "3277/3277 [==============================] - 0s - loss: 0.1219 - acc: 0.9628 - val_loss: 0.1493 - val_acc: 0.9680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f751b33d410>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=15, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.save_weights(path+'models/conv_512_7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.load_weights(path+'models/conv_512_7.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3277 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=5, height_shift_range=0.1, \n",
    "                shear_range=0.05, channel_shift_range=10, width_shift_range=0.1)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "gen = image.ImageDataGenerator()\n",
    "val_batches = get_batches(path+'valid', gen, batch_size=batch_size*2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bnl =  get_bn_layers(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for l in bnl: conv_model.add(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for l1,l2 in zip(bn_model.layers, bnl): l2.set_weights(l1.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv_model.compile(Adam(), 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3277/3277 [==============================] - 32s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0052268964638014156, 0.99816905706438819]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.evaluate(trn, trn_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv_model.optimizer.lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "3277/3277 [==============================] - 48s - loss: 0.3208 - acc: 0.8938 - val_loss: 0.1649 - val_acc: 0.9480\n",
      "Epoch 2/7\n",
      "3277/3277 [==============================] - 41s - loss: 0.3184 - acc: 0.8956 - val_loss: 0.1543 - val_acc: 0.9540\n",
      "Epoch 3/7\n",
      "3277/3277 [==============================] - 41s - loss: 0.2840 - acc: 0.9027 - val_loss: 0.1470 - val_acc: 0.9560\n",
      "Epoch 4/7\n",
      "3277/3277 [==============================] - 41s - loss: 0.2772 - acc: 0.9033 - val_loss: 0.1590 - val_acc: 0.9580\n",
      "Epoch 5/7\n",
      "3277/3277 [==============================] - 41s - loss: 0.2730 - acc: 0.9060 - val_loss: 0.1615 - val_acc: 0.9460\n",
      "Epoch 6/7\n",
      "3277/3277 [==============================] - 41s - loss: 0.3072 - acc: 0.8941 - val_loss: 0.1453 - val_acc: 0.9640\n",
      "Epoch 7/7\n",
      "3277/3277 [==============================] - 41s - loss: 0.2763 - acc: 0.9027 - val_loss: 0.1458 - val_acc: 0.9620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8d682fe8d0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.fit_generator(batches, batches.N, nb_epoch=14, \n",
    "                         validation_data=val_batches, nb_val_samples=val_batches.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "3277/3277 [==============================] - 49s - loss: 0.2530 - acc: 0.9112 - val_loss: 0.1470 - val_acc: 0.9620\n",
      "Epoch 2/7\n",
      "3277/3277 [==============================] - 41s - loss: 0.2539 - acc: 0.9109 - val_loss: 0.1318 - val_acc: 0.9660\n",
      "Epoch 3/7\n",
      "3277/3277 [==============================] - 41s - loss: 0.2422 - acc: 0.9231 - val_loss: 0.1613 - val_acc: 0.9500\n",
      "Epoch 4/7\n",
      "3277/3277 [==============================] - 41s - loss: 0.2669 - acc: 0.9081 - val_loss: 0.1300 - val_acc: 0.9660\n",
      "Epoch 5/7\n",
      "3277/3277 [==============================] - 41s - loss: 0.2255 - acc: 0.9213 - val_loss: 0.1483 - val_acc: 0.9600\n",
      "Epoch 6/7\n",
      "3277/3277 [==============================] - 41s - loss: 0.2091 - acc: 0.9298 - val_loss: 0.1374 - val_acc: 0.9620\n",
      "Epoch 7/7\n",
      "3277/3277 [==============================] - 41s - loss: 0.2342 - acc: 0.9240 - val_loss: 0.1304 - val_acc: 0.9680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8d68269850>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.fit_generator(batches, batches.N, nb_epoch=7, \n",
    "                         validation_data=val_batches, nb_val_samples=val_batches.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv_model.save_weights(path+'models/conv_512_7_aug.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = conv_model.predict(test, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_batches = gen.flow(test, preds, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "val_batches = get_batches(path+'valid', batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3277 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(path+'train', gen_t, batch_size=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mi = MixIterator([batches, test_batches, val_batches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "4769/4777 [============================>.] - ETA: 0s - loss: 0.3337 - acc: 0.9006"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/keras/engine/training.py:1494: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4833/4777 [==============================] - 58s - loss: 0.3331 - acc: 0.9007 - val_loss: 0.1358 - val_acc: 0.9640\n",
      "Epoch 2/8\n",
      "4833/4777 [==============================] - 54s - loss: 0.3153 - acc: 0.9075 - val_loss: 0.1197 - val_acc: 0.9620\n",
      "Epoch 3/8\n",
      "4833/4777 [==============================] - 54s - loss: 0.2780 - acc: 0.9214 - val_loss: 0.1068 - val_acc: 0.9700\n",
      "Epoch 4/8\n",
      "4833/4777 [==============================] - 54s - loss: 0.3104 - acc: 0.9139 - val_loss: 0.0973 - val_acc: 0.9680\n",
      "Epoch 5/8\n",
      "4825/4777 [==============================] - 54s - loss: 0.2818 - acc: 0.9183 - val_loss: 0.2420 - val_acc: 0.9600\n",
      "Epoch 6/8\n",
      "4833/4777 [==============================] - 54s - loss: 0.2692 - acc: 0.9214 - val_loss: 0.0758 - val_acc: 0.9760\n",
      "Epoch 7/8\n",
      "4833/4777 [==============================] - 54s - loss: 0.2602 - acc: 0.9263 - val_loss: 0.0715 - val_acc: 0.9760\n",
      "Epoch 8/8\n",
      "4833/4777 [==============================] - 54s - loss: 0.2683 - acc: 0.9236 - val_loss: 0.0639 - val_acc: 0.9760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8d73c9b4d0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.fit_generator(mi, mi.N, nb_epoch=8, validation_data=(val, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/7, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val_preds = bn_model.predict(conv_val_feat, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.060490751042962074)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.metrics.categorical_crossentropy(val_labels, do_clip(val_preds, 0.98)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "sub_batches = get_batches(path+'test', shuffle=False, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv_test_feat = conv_model.predict_generator(sub_batches, sub_batches.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds = conv_model.predict_generator(sub_batches, sub_batches.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "subm = do_clip(preds,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "subm_name = path+'results/subm.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "classes = sorted(batches.class_indices, key=batches.class_indices.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_00005.jpg</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_00007.jpg</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.071907</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_00009.jpg</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_00018.jpg</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_00027.jpg</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.129066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image       ALB       BET       DOL       LAG       NoF     OTHER  \\\n",
       "0  img_00005.jpg  0.042857  0.042857  0.042857  0.042857  0.700000  0.042857   \n",
       "1  img_00007.jpg  0.700000  0.042857  0.042857  0.071907  0.042857  0.042857   \n",
       "2  img_00009.jpg  0.700000  0.042857  0.042857  0.042857  0.042857  0.042857   \n",
       "3  img_00018.jpg  0.700000  0.042857  0.042857  0.042857  0.042857  0.042857   \n",
       "4  img_00027.jpg  0.700000  0.042857  0.042857  0.042857  0.042857  0.042857   \n",
       "\n",
       "      SHARK       YFT  \n",
       "0  0.042857  0.042857  \n",
       "1  0.042857  0.042857  \n",
       "2  0.042857  0.042857  \n",
       "3  0.042857  0.042857  \n",
       "4  0.042857  0.129066  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'image', [a[4:] for a in sub_batches.filenames])\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(subm_name, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='data/fish/results/subm.gz' target='_blank'>data/fish/results/subm.gz</a><br>"
      ],
      "text/plain": [
       "/data/jhoward/fast-image/nbs/data/fish/results/subm.gz"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink(subm_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
